{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Wrangle Open Street Map Data\n",
    "## Data Wrangling for Auditing and Cleaning\n",
    "**Location: Miami, Florida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandalwood 1\n",
      "Northeast 28\n",
      "Sunflower 1\n",
      "Waterbrook 1\n",
      "Nandina 1\n",
      "Southfields 1\n",
      "Saint 3\n",
      "Eastlake 1\n",
      "Whitaker 1\n",
      "Worth 1\n",
      "Entrada 1\n",
      "Southwest 53\n",
      "Windmill 2\n",
      "Stanton 2\n",
      "Equestrian 2\n",
      "Sacramento 1\n",
      "Elm 1\n",
      "Stallion 1\n",
      "Seabreeze 3\n",
      "Nob 1\n",
      "Simonton 1\n",
      "Emerald 2\n",
      "Sequoia 1\n",
      "World 1\n",
      "southwest 1\n",
      "Equine 1\n",
      "Saratoga 1\n",
      "Newport 1\n",
      "Elderberry 1\n",
      "Southern 1\n",
      "Shadyside 1\n",
      "S.W. 1\n",
      "Ensenada 1\n",
      "Scripps 1\n",
      "San 6\n",
      "Savannah 1\n",
      "Sevilla 1\n",
      "Washington 2\n",
      "Spoonbill 1\n",
      "Seville 1\n",
      "Nighthawk 2\n",
      "Springside 2\n",
      "Westwood 1\n",
      "Windsor 1\n",
      "Sunrise 1\n",
      "Whitehead 1\n",
      "Willow 4\n",
      "West 26\n",
      "Silks 1\n",
      "Northpark 1\n",
      "State 3\n",
      "Sportsplex 1\n",
      "Northwest 64\n",
      "Seminole 4\n",
      "Seagrape 1\n",
      "East 15\n",
      "Norhwest 1\n",
      "Schooner 1\n",
      "Wiles 1\n",
      "E. 2\n",
      "Swansea 1\n",
      "Water 3\n",
      "Southeast 16\n",
      "N.W. 1\n",
      "Stillwater 1\n",
      "Stone 1\n",
      "Northumberland 1\n",
      "Silverbell 1\n",
      "S 13\n",
      "Slippery 1\n",
      "W 13\n",
      "Seneca 1\n",
      "Sorrento 1\n",
      "Westgate 1\n",
      "Sunnyland 1\n",
      "Sand 2\n",
      "Sandpiper 1\n",
      "SE 1\n",
      "Northlake 1\n",
      "Smoke 1\n",
      "Waterview 3\n",
      "Sailboat 1\n",
      "Winterberry 1\n",
      "Saddlebrook 1\n",
      "Stonebrook 1\n",
      "S. 1\n",
      "Winfield 1\n",
      "Waterford 1\n",
      "Sirius 1\n",
      "Staghorn 1\n",
      "North 23\n",
      "Waters 2\n",
      "Sunny 2\n",
      "NE 5\n",
      "Sabal 3\n",
      "Eller 1\n",
      "Woodfall 1\n",
      "Westroads 1\n",
      "N. 3\n",
      "Saddle 2\n",
      "Stonemont 2\n",
      "N/A 1\n",
      "St 2\n",
      "Salerno 1\n",
      "Summit 1\n",
      "Sunset 4\n",
      "Enterprise 1\n",
      "Endeavour 1\n",
      "Stroller 1\n",
      "Savanna 1\n",
      "Surrey 1\n",
      "Spinnaker 1\n",
      "Springtree 1\n",
      "Sago 2\n",
      "SW 45\n",
      "Windward 2\n",
      "Shiloh 1\n",
      "Nautica 1\n",
      "Eastwood 1\n",
      "Edgewater 3\n",
      "NW 19\n",
      "Sanderling 1\n",
      "Sansburys 1\n",
      "Waterside 1\n",
      "Estancia 2\n",
      "Stables 1\n",
      "Egret 3\n",
      "Espanola 1\n",
      "Somerset 1\n",
      "Spyglass 1\n",
      "Eagle 9\n",
      "Sorrel 1\n",
      "Stirrup 1\n",
      "Santa 1\n",
      "Silver 1\n",
      "Sheridan 1\n",
      "E 11\n",
      "Sheltingham 1\n",
      "Shotgun 1\n",
      "Sycamore 1\n",
      "Stirling 1\n",
      "Wellington 3\n",
      "Sapphire 11\n",
      "N 13\n",
      "South 27\n",
      "Skylark 1\n",
      "Sanctuary 1\n",
      "Sparrow 1\n",
      "Executive 2\n",
      "sw 1\n",
      "wellington 1\n",
      "Walkers 1\n",
      "Westward 1\n",
      "Weston 3\n",
      "Sweetgum 1\n",
      "Northpoint 1\n",
      "Silktree 1\n",
      "Worthington 1\n",
      "Seabay 1\n",
      "Nelson 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Auditing and cleaning the data\n",
    "\"\"\"\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import csv\n",
    "\n",
    "# File from OpenStreetMap.org for Miami, Florida, USA. Size: 353.9 MB.\n",
    "OSMFILE = \"miami_florida.osm\"\n",
    "\n",
    "# Audit regex objects to parse out directions and street types and assess the validity of zip codes\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE) # Street types grabs the last word from the street name\n",
    "direction_re = re.compile(r'^[NSEW]\\S*\\s', re.IGNORECASE) # Direction grabs the first word/letter from the street name\n",
    "zip_re = re.compile(r'^33\\d{3}($|(-\\d{4})$)', re.IGNORECASE) # Zip looks for 5-digits starting with 33, allows for +4\n",
    "\n",
    "# List of common street names in Miami and elsewhere in South Florida\n",
    "expected_st = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"Run\", \"Terrace\", \"Highway\", \"Trace\", \"Plaza\",\n",
    "            \"Causeway\", \"Path\", \"Isle\", \"Lake\", \"Row\", \"Crescent\", \"Manor\"]\n",
    "\n",
    "# Because the list of possible street types is so large, it was easier to maintain it in a separate CSV file\n",
    "# generate_standardized_street_mapping is a function that reads in this file for mapping purposes during the audit\n",
    "# This is the function that reads in the street type mapping CSV file and stores it in a dictionary called \"mappings\"\n",
    "def generate_standardized_street_mapping(street_type_file):\n",
    "    mappings = {}\n",
    "    fieldnames = [\"abbreviation\", \"preferred\"]\n",
    "    with open(street_type_file, \"rU\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile, fieldnames=fieldnames)\n",
    "        for row in reader:\n",
    "            mappings[row[\"abbreviation\"]] = row[\"preferred\"]\n",
    "    csvfile.close()\n",
    "    return mappings\n",
    "                      \n",
    "STREET_TYPE_CSV = \"standardized_street_types.csv\"\n",
    "st_mapping = generate_standardized_street_mapping(STREET_TYPE_CSV)    \n",
    "\n",
    "\n",
    "# List of common directions assigned to streets in Miami and elsewhere in South Florida\n",
    "#expected_dir = [\"Northwest\", \"Northeast\", \"Southwest\", \"Southeast\", \"North\", \"South\", \"East\", \"West\"]\n",
    "expected_dir = []\n",
    "\n",
    "# After iterating through the audit, I discovered the following non-standard directional names need to be changed\n",
    "dir_mapping = { \"NW\" : \"Northwest\",\n",
    "                \"NE\" : \"Northeast\",\n",
    "                \"SW\" : \"Southwest\",\n",
    "                \"SE\" : \"Southeast\",\n",
    "                \"N.W.\" : \"Northwest\",\n",
    "                \"Norhwest\" : \"Northwest\",\n",
    "                \"sw\" : \"Southwest\",\n",
    "                \"southwest\" : \"Southwest\",\n",
    "                \"S.W.\" : \"Southwest\",\n",
    "                \"N.\" : \"North\",\n",
    "                \"S.\" : \"South\",\n",
    "                \"E.\" : \"East\",\n",
    "                \"W.\" : \"West\",\n",
    "                \"N\" : \"North\",\n",
    "                \"S\" : \"South\",\n",
    "                \"E\" : \"East\",\n",
    "                \"W\" : \"West\",\n",
    "                \"wellington\" : \"Wellington\" # Found this during the audit and threw it in here to capitalize the W               \n",
    "                 }\n",
    "\n",
    "# In going through the list of bad zip codes generated by my audit, I found a few that needed to be mapped \n",
    "# based on the results of a few manual Google map searches\n",
    "zip_mapping = { \"11890\" : \"33181\",\n",
    "               \"3331\" : \"33313\",\n",
    "               \"3447\" : \"33407\",\n",
    "               \"(561) 795-4333\" : \"33411\",\n",
    "               \"0\" : \"33326\",\n",
    "               u'361-0529\\u200e' : \"33431\",\n",
    "               \"FL\" : \"33185\"\n",
    "              }\n",
    "\n",
    "# Two global sets to store the address parts (street type and direction) that the audit picked up, but \n",
    "# were not changed\n",
    "dir_not_found = set()\n",
    "st_not_found = set()\n",
    "\n",
    "\n",
    "# This is a generic function to check a specific part of the street name, whether it is the direction on the \n",
    "# front of the street name or the street type at the end. This is called from audit_street_type and audit_direction\n",
    "def audit_address_part(regex_obj, types, street_name, expected):\n",
    "    a = regex_obj.search(street_name)\n",
    "    if a:\n",
    "        address_part = a.group().strip()\n",
    "        if address_part not in expected:\n",
    "            types[address_part].add(street_name)\n",
    "            \n",
    "\n",
    "# This function calls audit_address_part specifically to audit a street type            \n",
    "def audit_street_type(street_types, street_name):\n",
    "    audit_address_part(street_type_re, street_types, street_name, expected_st)\n",
    "    \n",
    "\n",
    "# This function calls audit_address_part specifically to audit a direction name    \n",
    "def audit_direction(dir_types, street_name):\n",
    "    audit_address_part(direction_re, dir_types, street_name, expected_dir)\n",
    "            \n",
    "\n",
    "# This function determines if an XML tag is a street name\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "# This function determines if an XML tag is a state abbreviation\n",
    "def is_state(elem):\n",
    "    return (elem.attrib['k'] == \"addr:state\")\n",
    "\n",
    "\n",
    "# This function loops through all of the street names in the OSM file and outputs dictionaries for all\n",
    "# possible street types and directions, coupled with an array of all streets names found with those characteristics\n",
    "def audit_street_names(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    dir_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "                    audit_direction(dir_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types, dir_types\n",
    "\n",
    "\n",
    "# This function determins if an XML tag refers to a zip code (aka \"postcode\")\n",
    "def is_zip(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "# This function takes a zip_code and a \"bad_zips\" list as inputs. If the zip code does not match the \n",
    "# regular expression for proper zip code formatting, the zip code is written to the bad_zips list\n",
    "def audit_zip(bad_zips, zip_code):\n",
    "    z = zip_re.search(zip_code)\n",
    "    if not z:\n",
    "        bad_zips.add(zip_code)\n",
    "\n",
    "\n",
    "# This function runs through the entire OSM file looking for any \"bad zips\" as determined by audit_zip above, and \n",
    "# returns the entire list of bad zip codes that it finds\n",
    "def audit_zips(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    bad_zips = set()\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zip(tag):\n",
    "                    audit_zip(bad_zips, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return bad_zips\n",
    "\n",
    "\n",
    "# This function is a generic function that reads either the direction (at the front of the string) or the \n",
    "# street type (and the end of the string), determines if it is erroneous, and if so, corrects the value of \n",
    "# that part of the string before returning the whole string\n",
    "def update_address_part(regex_obj, name, mapping, category):\n",
    "    \n",
    "    # This function behaves slightly differently if we are dealing with a street type versus a direction.\n",
    "    # The replacement strings needs a leading space for a street type and a trailing space for a direction.\n",
    "    # Also, the global sets that store the items that were \"not found\" in the mapping have different names.\n",
    "    if category == \"street_type\":\n",
    "        expected = expected_st\n",
    "        not_found_set = st_not_found # Removed for the actual update process\n",
    "    elif category == \"direction\":\n",
    "        expected = expected_dir\n",
    "        not_found_set = dir_not_found # Removed for the actual update process\n",
    "    \n",
    "    # First, search for a string that matches the regex expression in the street name\n",
    "    address_part = regex_obj.search(name)\n",
    "    \n",
    "    # Grab the string that is found and remove any whitespace\n",
    "    if address_part:\n",
    "        a = address_part.group().strip()\n",
    "        \n",
    "        # Check to if an unexpected string that we found is in the mapping. If so, replace it with the value \n",
    "        # found at that key in the mapping.\n",
    "        if a not in expected:\n",
    "            if a in mapping.keys():\n",
    "                new_address_part = mapping[a]\n",
    "                if category == \"street_type\": \n",
    "                    start, middle, finish = name.rpartition(\" \") \n",
    "                    name = start + middle + new_address_part\n",
    "                elif category == \"direction\":\n",
    "                    start, middle, finish = name.partition(\" \")\n",
    "                    name = new_address_part + middle + finish\n",
    "                \n",
    "            # Otherwise, keep track of the address parts that were not mapped for auditing purposes.\n",
    "            # (Removed for the actual update process)\n",
    "            else:\n",
    "                not_found_set.add(a)\n",
    "            \n",
    "    return name\n",
    "\n",
    "# This function calls update_address_part twice, once for the street type and once for the direction and \n",
    "# returns the corrected name\n",
    "def update_street_name(name, st_mapping, dir_mapping):\n",
    "    \n",
    "    # For a given street name, first update the street type according to the street type mapping\n",
    "    name = update_address_part(street_type_re, name, st_mapping, \"street_type\")\n",
    "    \n",
    "    # Then update the direction according to the direction mapping\n",
    "    name = update_address_part(direction_re, name, dir_mapping, \"direction\")\n",
    "    \n",
    "    # When done, return the street name with its street type and/or direction fixed\n",
    "    return name\n",
    "\n",
    "\n",
    "# This function works on improperly formatted zip codes to assign mapping values, strip out spaces, or strip out\n",
    "# inappropriate state abbreviations\n",
    "def update_zip(zip_code, zip_mapping):\n",
    "    z = zip_re.search(zip_code)\n",
    "    if not z:\n",
    "        if zip_code in zip_mapping.keys():\n",
    "            zip_code = zip_mapping[zip_code]\n",
    "        elif zip_re.search(zip_code.strip()):\n",
    "            zip_code = zip_code.strip()\n",
    "        else:\n",
    "            zip_code = zip_code.strip(\"FL-\").strip(\"Fl\").strip(\"FL\").strip()\n",
    "    return zip_code\n",
    "\n",
    "st_types, dir_types = audit_street_names(OSMFILE)\n",
    "#pprint.pprint(st_types)\n",
    "\n",
    "\n",
    "# In order to determine which naming convention to use, I needed to define an \"expected direction\" array. \n",
    "# I started looking at the count information from auditing the directions\n",
    "for d in dir_types:\n",
    "    if d[0] in \"NESWnesw\":\n",
    "        print d, len(dir_types[d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Northwest 64\n",
      "Northeast 28\n",
      "Southwest 53\n",
      "Southeast 16\n",
      "southwest 1\n",
      "N.W. 1\n",
      "S.W. 1\n",
      "NW 19\n",
      "NE 5\n",
      "SW 45\n",
      "SE 1\n",
      "sw 1\n",
      "North 23\n",
      "South 27\n",
      "East 15\n",
      "West 26\n",
      "N 13\n",
      "S 13\n",
      "E 11\n",
      "W 13\n",
      "N. 3\n",
      "S. 1\n",
      "E. 2\n"
     ]
    }
   ],
   "source": [
    "# From there, I manually picked out all possible direction names and put them in a list called \"direction_names\"\n",
    "direction_names = [\n",
    "    \"Northwest\", \"Northeast\", \"Southwest\", \"Southeast\",\n",
    "    \"northwest\", \"northeast\", \"southwest\", \"southeast\",\n",
    "    \"N.W.\", \"N.E.\", \"S.W.\", \"S.E.\",\n",
    "    \"NW\", \"NE\", \"SW\", \"SE\",\n",
    "    \"nw\", \"ne\", \"sw\", \"se\",\n",
    "    \"North\", \"South\", \"East\", \"West\",\n",
    "    \"N\", \"S\", \"E\", \"W\",\n",
    "    \"N.\", \"S.\", \"E.\", \"W.\"\n",
    "]\n",
    "\n",
    "# Using that array helped me isolate the counts of just the direction names\n",
    "# Based on those results (below), I chose to use the full direction name instead of the abbreviation\n",
    "for d in direction_names:\n",
    "    if d in dir_types:\n",
    "        print d, len(dir_types[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of some of the street names that were considered for cleaning and the result of cleaning\n",
      "(truncated for the sake of brevity):\n",
      "Sunset Springs Spgs => Sunset Springs Springs\n",
      "Capistrano => Capistrano\n",
      "Country Isles Rd => Country Isles Road\n",
      "East Las Olas BLVD => East Las Olas Boulevard\n",
      "Osprey Bnd => Osprey Bend\n",
      "US Highway 1 => US Highway 1\n",
      "Hwy 27 => Hwy 27\n",
      "Alhambra Cirlce => Alhambra Circle\n",
      "Sacramento => Sacramento\n",
      "E McNab RD => East McNab Road\n",
      "Atlanta => Atlanta\n",
      "Sabal Trl => Sabal Trail\n",
      "Augusta => Augusta\n",
      "E Merion => East Merion\n",
      "Via Poinciana => Via Poinciana\n",
      "826 => 826\n",
      "San Luis Rey => San Luis Rey\n",
      "Water Pt => Water Point\n",
      "Marina Bay Drive West => Marina Bay Drive West\n",
      "Country Club Cres => Country Club Crescent\n",
      "Birkdale => Birkdale\n",
      "184th street => 184th Street\n",
      "Broken Sound Parkway Northwest => Broken Sound Parkway Northwest\n",
      "Oakmont => Oakmont\n",
      "3196 N. Jog Rd. => 3196 N. Jog Road\n",
      "11860 W State Road 84 => 11860 W State Road 84\n",
      "Fox Holw => Fox Hollow\n",
      "Biscayne Blvd Ste 918 => Biscayne Blvd Ste 918\n",
      "North State Road 7 => North State Road 7\n",
      "Jefferson Ave. => Jefferson Avenue\n",
      "NW 20th Street, #108 => Northwest 20th Street, #108\n",
      "Columbia => Columbia\n",
      "SW 22nd Ave #101 => Southwest 22nd Ave #101\n",
      "Tequesta Trce => Tequesta Trace\n",
      "Linton Blvd Suite 104 => Linton Blvd Suite 104\n",
      "Seabreeze Blvd. => Seabreeze Boulevard\n",
      "US Hwy-1 => US Hwy-1\n",
      "Sapphire Mnr => Sapphire Manor\n",
      "5979 => 5979\n",
      "28450 SW 152ND AVE => 28450 SW 152ND Avenue\n",
      "Huntington => Huntington\n",
      "Longview => Longview\n",
      "NW 58th St. => Northwest 58th Street\n",
      "Harbor Way => Harbor Way\n",
      "Medinah => Medinah\n",
      "South Dixie Highway #177 => South Dixie Highway #177\n",
      "Wentworth => Wentworth\n",
      "140th Avenue North => 140th Avenue North\n",
      "Inverness => Inverness\n",
      "Alton Road, Suite 710 => Alton Road, Suite 710\n",
      "29901 S DIXIE HWY => 29901 S DIXIE Highway\n",
      "Southwest 190th Street, #2111 => Southwest 190th Street, #2111\n",
      "Broken Sound Pky NW => Broken Sound Pky NW\n",
      "Sunny Isles Blvd Ste => Sunny Isles Blvd Suite\n",
      "Via Christina => Via Christina\n",
      "84 Sr => 84 Sr\n",
      "N/A N/A N/A => N/A N/A N/A\n",
      "W 49 St Suite 8,9,10 => West 49 St Suite 8,9,10\n",
      "SW 88th St => Southwest 88th Street\n",
      "Westbrook => Westbrook\n",
      "Charleston => Charleston\n",
      "Spinnaker => Spinnaker\n",
      "Harding Ave #19 => Harding Ave #19\n",
      "12970 SW 268TH ST => 12970 SW 268TH Street\n",
      "Biscayne Boulevard Aventura => Biscayne Boulevard Aventura\n",
      "SW 63rd Ave => Southwest 63rd Avenue\n",
      "Fairway Ter => Fairway Terrace\n",
      "Sapphire Cv => Sapphire Cove\n",
      "Laurel Ct => Laurel Court\n",
      "Heron Ridge Ln => Heron Ridge Lane\n",
      "southwest 112th ave => Southwest 112th Avenue\n",
      "Muirfield => Muirfield\n",
      "Somerset => Somerset\n",
      "Spyglass => Spyglass\n",
      "Plaza Real => Plaza Real\n",
      "E. Commercial Blvd. Ste. 322 => East Commercial Blvd. Ste. 322\n",
      "Equestrian Club Druve => Equestrian Club Drive\n",
      "Blatt Pl => Blatt Place\n",
      "S Dixie Hwy => South Dixie Highway\n",
      "State Road 9 => State Road 9\n",
      "Biscayne Boulevard, Suite 328 => Biscayne Boulevard, Suite 328\n",
      "Sportsplex Dr => Sportsplex Drive\n",
      "324 Datura St #207 => 324 Datura St #207\n",
      "North University Drive Ste 206 => North University Drive Ste 206\n",
      "Sheridan Street, Ste 209 => Sheridan Street, Ste 209\n",
      "Raleigh => Raleigh\n",
      "6900 Miramar Pkwy => 6900 Miramar Parkway\n",
      "46th Lane South => 46th Lane South\n",
      "Seagrape Cir => Seagrape Circle\n",
      "NW 27th st => Northwest 27th Street\n",
      "Baton Rouge => Baton Rouge\n",
      "Dade Blvd => Dade Boulevard\n",
      "---\n",
      "Address last words that are potentially street names but were not fixed:\n",
      "set(['Rey', 'Capistrano', 'West', '710', 'Charleston', 'Birkdale', 'Northwest', 'Medinah', '177', 'Oakmont', 'Muirfield', 'Somerset', '84', 'Wentworth', 'Real', '322', '27', 'Columbia', 'Inverness', 'Sacramento', 'Aventura', '328', '1', '918', '7', 'Merion', '2111', 'Atlanta', 'NW', 'Christina', '207', '206', '209', 'Sr', 'N/A', '8,9,10', 'Augusta', 'South', '9', '108', 'Raleigh', 'Spyglass', 'Westbrook', 'North', '101', 'Poinciana', '104', 'Spinnaker', 'Hwy-1', '19', '5979', 'Rouge', 'Huntington', '826', 'Longview'])\n"
     ]
    }
   ],
   "source": [
    "st_types, dir_types = audit_street_names(OSMFILE)\n",
    "\n",
    "print \"Examples of some of the street names that were considered for cleaning and the result of cleaning\"\n",
    "print \"(truncated for the sake of brevity):\"\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    i = 0\n",
    "    for name in ways:\n",
    "        better_name = update_street_name(name, st_mapping, dir_mapping)\n",
    "        if i % 4800 == 0:\n",
    "            print name, \"=>\", better_name\n",
    "            i = i + 1\n",
    "\n",
    "print \"---\"\n",
    "print \"Address last words that are potentially street names but were not fixed:\"\n",
    "print st_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33314  => 33314\n",
      "FL 33487-3536 => 33487-3536\n",
      "FL 33431 => 33431\n",
      "FL 33433 => 33433\n",
      "FL 33026 => 33026\n",
      "361-0529‎ => 33431\n",
      "FL 33033 => 33033\n",
      "FL 33126 => 33126\n",
      "Fl 33186 => 33186\n",
      "FL  33351 => 33351\n",
      "FL 33431-4403 => 33431-4403\n",
      "FL => 33185\n",
      "FL 33166 => 33166\n",
      "0 => 33326\n",
      "FL 33312 => 33312\n",
      "FL-33140 => 33140\n",
      "FL 33134 => 33134\n",
      "FL 33016 => 33016\n",
      "3331 => 33313\n",
      "FL 33012 => 33012\n",
      "11890 => 33181\n",
      "FL 33140 => 33140\n",
      "FL33401 => 33401\n",
      "(561) 795-4333 => 33411\n",
      "FL 33322 => 33322\n",
      "FL-33139 => 33139\n",
      "3447 => 33407\n",
      "FL 33131 => 33131\n",
      "---\n",
      "Address first words that are potentially directions but were not fixed:\n",
      "set(['Sandalwood', 'Southfields', 'Sunflower', 'Waterbrook', 'Nandina', 'Saint', 'Eastlake', 'Whitaker', 'Worth', 'Entrada', 'Windmill', 'Stanton', 'Equestrian', 'Sacramento', 'Waterview', 'Stallion', 'Seabreeze', 'Nob', 'Simonton', 'Emerald', 'Sequoia', 'World', 'Equine', 'Saratoga', 'Newport', 'Elderberry', 'Shadyside', 'Ensenada', 'San', 'Savannah', 'Sevilla', 'Washington', 'Spoonbill', 'Seville', 'Nighthawk', 'Springside', 'Westwood', 'Sandpiper', 'Sunrise', 'Whitehead', 'Willow', 'Eller', 'Silks', 'Stonemont', 'State', 'Sportsplex', 'Seminole', 'Seagrape', 'Swansea', 'Schooner', 'Wiles', 'Sheltingham', 'Water', 'Scripps', 'Stillwater', 'Stone', 'Northumberland', 'Silverbell', 'Sand', 'Slippery', 'Smoke', 'Seneca', 'Sorrento', 'Westgate', 'Sunnyland', 'Windsor', 'Northlake', 'Elm', 'Sailboat', 'Winterberry', 'Saddlebrook', 'Stonebrook', 'Winfield', 'Waterford', 'Sirius', 'Staghorn', 'Waters', 'Southern', 'Sabal', 'Nelson', 'Woodfall', 'Westroads', 'Saddle', 'Northpark', 'N/A', 'St', 'Salerno', 'Summit', 'Sunset', 'Enterprise', 'Endeavour', 'Stroller', 'Savanna', 'Surrey', 'Spinnaker', 'Springtree', 'Sago', 'Windward', 'Shiloh', 'Nautica', 'Eastwood', 'Edgewater', 'Sansburys', 'Waterside', 'Estancia', 'Stables', 'Egret', 'Espanola', 'Somerset', 'Spyglass', 'Eagle', 'Sorrel', 'Stirrup', 'Santa', 'Silver', 'Sheridan', 'Sunny', 'Shotgun', 'Sycamore', 'Stirling', 'Wellington', 'Sapphire', 'Skylark', 'Sanctuary', 'Sparrow', 'Executive', 'Walkers', 'Westward', 'Weston', 'Sweetgum', 'Northpoint', 'Silktree', 'Worthington', 'Seabay', 'Sanderling'])\n"
     ]
    }
   ],
   "source": [
    "# List of common directions assigned to streets in Miami and elsewhere in South Florida\n",
    "expected_dir = [\"Northwest\", \"Northeast\", \"Southwest\", \"Southeast\", \"North\", \"South\", \"East\", \"West\"]\n",
    "dir_not_found = set()\n",
    "st_types, dir_types = audit_street_names(OSMFILE)\n",
    "for dir_type, ways in dir_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_street_name(name, st_mapping, dir_mapping)\n",
    "#        print name, \"=>\", better_name\n",
    "\n",
    "zip_codes = audit_zips(OSMFILE)\n",
    "    \n",
    "for zip in zip_codes:\n",
    "    better_zip = update_zip(zip, zip_mapping)\n",
    "    print zip, \"=>\", better_zip\n",
    "    \n",
    "print \"---\"\n",
    "print \"Address first words that are potentially directions but were not fixed:\"\n",
    "print dir_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_address_part(regex_obj, name, mapping, category):\n",
    "    \n",
    "    # This function behaves slightly differently if we are dealing with a street type versus a direction.\n",
    "    # The replacement strings needs a leading space for a street type and a trailing space for a direction.\n",
    "    # Also, the global sets that store the items that were \"not found\" in the mapping have different names.\n",
    "    if category == \"street_type\":\n",
    "        expected = expected_st\n",
    "        #not_found_set = st_not_found # REMOVED FOR THE ACTUAL UPDATE PROCESS\n",
    "    elif category == \"direction\":\n",
    "        expected = expected_dir\n",
    "        #not_found_set = dir_not_found # Removed for the actual update process\n",
    "    \n",
    "    # First, search for a string that matches the regex expression in the street name\n",
    "    address_part = regex_obj.search(name)\n",
    "    \n",
    "    # Grab the string that is found and remove any whitespace\n",
    "    if address_part:\n",
    "        a = address_part.group().strip()\n",
    "        \n",
    "        # Check to if an unexpected string that we found is in the mapping. If so, replace it with the value \n",
    "        # found at that key in the mapping.\n",
    "        if a not in expected:\n",
    "            if a in mapping.keys():\n",
    "                new_address_part = mapping[a]\n",
    "                if category == \"street_type\":\n",
    "                    start, middle, finish = name.rpartition(\" \")\n",
    "                    name = start + middle + new_address_part\n",
    "                elif category == \"direction\":\n",
    "                    start, middle, finish = name.partition(\" \")\n",
    "                    name = new_address_part + middle + finish\n",
    "                \n",
    "            # Otherwise, keep track of the address parts that were not mapped for auditing purposes.\n",
    "            # (Removed for the actual update process)\n",
    "            # else:\n",
    "            #     not_found_set.add(a)\n",
    "            \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672263\n",
      "{'created': {'changeset': '7463675',\n",
      "             'timestamp': '2011-03-05T14:23:44Z',\n",
      "             'uid': '369983',\n",
      "             'user': 'grouper',\n",
      "             'version': '3'},\n",
      " 'id': '95946810',\n",
      " 'pos': [26.0593811, -80.133733],\n",
      " 'type': 'node'}\n",
      "{'created': {'changeset': '34254691',\n",
      "             'timestamp': '2015-09-26T00:11:14Z',\n",
      "             'uid': '2679756',\n",
      "             'user': 'MountainAddict',\n",
      "             'version': '1'},\n",
      " 'highway': 'service',\n",
      " 'id': '372411959',\n",
      " 'node_refs': ['3759672684',\n",
      "               '3759668981',\n",
      "               '3759668982',\n",
      "               '3759668983',\n",
      "               '3759668984',\n",
      "               '3759668983',\n",
      "               '3759672590',\n",
      "               '3759672685',\n",
      "               '3759687160',\n",
      "               '3759672672'],\n",
      " 'service': 'parking_aisle',\n",
      " 'type': 'way'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\"\"\"\n",
    "Your task is to wrangle the data, transform the shape of the data, and export a list \n",
    "of dictionaries to a JSON file to be imported into MongoDB.\n",
    "\n",
    "You have to complete the function 'shape_element'.\n",
    "We have provided a function that will parse the map file, and call the function with the element\n",
    "as an argument. You should return a dictionary, containing the shaped data for that element.\n",
    "We have also provided a way to save the data in a file, so that you could use\n",
    "mongoimport later on to import the shaped data into MongoDB. \n",
    "\n",
    "If you are using this code in your final project, you are strongly encouraged to use the code \n",
    "from previous exercise to update the street names before you save them to JSON. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # YOUR CODE HERE\n",
    "        # First grab the element's tag and call that the node's \"type\"\n",
    "        node[\"type\"] = element.tag\n",
    "\n",
    "        # Loop through the top level keys\n",
    "        top_level_keys = element.attrib.keys()\n",
    "        created = {}\n",
    "        lat = \"\"\n",
    "        lon = \"\"\n",
    "        for tlk in top_level_keys:\n",
    "            if tlk == \"lat\":\n",
    "                lat = float(element.get(tlk))\n",
    "            elif tlk == \"lon\":\n",
    "                lon = float(element.get(tlk))\n",
    "            elif tlk in CREATED:\n",
    "                created[tlk] = element.get(tlk)\n",
    "            else:\n",
    "                node[tlk] = element.get(tlk)\n",
    "\n",
    "        node[\"created\"] = created\n",
    "        if lat != \"\" or lon != \"\":\n",
    "            node[\"pos\"] = [lat, lon] \n",
    "        \n",
    "        address_info = {}\n",
    "        # Now loop through tag's\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            p = problemchars.match(tag.attrib[\"k\"])\n",
    "            if p == None:\n",
    "                k_array = tag.attrib[\"k\"].split(\":\")\n",
    "                if k_array[0] == \"addr\":\n",
    "                    if len(k_array) == 2: \n",
    "                        if is_street_name(tag):\n",
    "                            val = update_street_name(tag.attrib[\"v\"], st_mapping, dir_mapping)\n",
    "                        elif is_zip(tag):\n",
    "                            val = update_zip(tag.attrib[\"v\"], zip_mapping)\n",
    "                        elif is_state(tag):\n",
    "                            val = \"FL\"\n",
    "                        else:\n",
    "                            val = tag.attrib[\"v\"]\n",
    "                        address_info[k_array[1]] = val\n",
    "                else:\n",
    "                    node[tag.attrib[\"k\"]] = tag.attrib[\"v\"]\n",
    "            \n",
    "        if len(address_info.keys()) > 0:\n",
    "            node[\"address\"] = address_info\n",
    "                \n",
    "        # Now loop through nd's\n",
    "        node_refs = []\n",
    "        for nd in element.iter(\"nd\"):\n",
    "            node_refs.append(nd.attrib[\"ref\"])\n",
    "        if len(node_refs) > 0:\n",
    "            node[\"node_refs\"] = node_refs\n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    # NOTE: if you are running this code on your computer, with a larger dataset, \n",
    "    # call the process_map procedure with pretty=False. The pretty=True option adds \n",
    "    # additional spaces to the output, making it significantly larger.\n",
    "    data = process_map('miami_florida.osm', False)\n",
    "    print len(data)\n",
    "    pprint.pprint(data[10])\n",
    "    pprint.pprint(data[-10])\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of zipped OSM file (miami_florida.osm.bz2): 23.7 MB\n",
      "Size of unzipped OSM file (miami_florida.osm): 353.9 MB\n",
      "Size of JSON file (miami_florida.osm.json): 390.4 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_size = os.path.getsize(OSMFILE + \".bz2\")\n",
    "print \"Size of zipped OSM file (\" + OSMFILE + \".bz2): \" + \"{0:.1f}\".format(float(file_size)/1000000) + \" MB\"\n",
    "file_size = os.path.getsize(OSMFILE)\n",
    "print \"Size of unzipped OSM file (\" + OSMFILE + \"): \" + \"{0:.1f}\".format(float(file_size)/1000000) + \" MB\"\n",
    "file_size = os.path.getsize(OSMFILE + \".json\")\n",
    "print \"Size of JSON file (\" + OSMFILE + \".json): \" + \"{0:.1f}\".format(float(file_size)/1000000) + \" MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">mongoimport --host 127.0.0.1:27017 --db examples --collection miami --file miami_florida.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Establish a Pymongo connection with the MongoDB client\n",
    "from IPython.display import Image\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "db = get_db(\"openstreetmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1,477,183\n",
      "Number of ways: 194,130\n"
     ]
    }
   ],
   "source": [
    "nodes = db.miami.find({\"type\":\"node\"}).count()\n",
    "print \"Number of nodes: \" + \"{:,}\".format(nodes)\n",
    "ways = db.miami.find({\"type\":\"way\"}).count()\n",
    "print \"Number of ways: \" + \"{:,}\".format(ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('560d97699d36f37943cbec8a'),\n",
       " u'created': {u'changeset': u'15311293',\n",
       "  u'timestamp': u'2013-03-10T08:11:36Z',\n",
       "  u'uid': u'1214881',\n",
       "  u'user': u'ryandrake',\n",
       "  u'version': u'4'},\n",
       " u'id': u'95946794',\n",
       " u'pos': [26.2667982, -80.2219395],\n",
       " u'type': u'node'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.miami.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 1,014\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    { \"$group\" : { \"_id\": \"$created.user\"}},\n",
    "    { \"$group\" : { \"_id\":1, \"count\": {\"$sum\" : 1}}}\n",
    "\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print \"Number of unique users: \" + \"{:,}\".format(a[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouper: 299,770, 17.9% of total\n",
      "woodpeck_fixbot: 236,866, 14.2% of total\n",
      "Latze: 137,610, 8.2% of total\n",
      "freebeer: 78,721, 4.7% of total\n",
      "carciofo: 72,054, 4.3% of total\n",
      "bot-mode: 62,272, 3.7% of total\n",
      "NE2: 59,798, 3.6% of total\n",
      "westendguy: 49,731, 3.0% of total\n",
      "Seandebasti: 48,397, 2.9% of total\n",
      "georafa: 39,831, 2.4% of total\n"
     ]
    }
   ],
   "source": [
    "ct = db.miami.count() # Total count of collection\n",
    "pipeline = [\n",
    "    { \"$group\" : { \"_id\": \"$created.user\",\n",
    "                 \"count\" : { \"$sum\" : 1 }}},\n",
    "    { \"$sort\" : { \"count\" : -1 }},\n",
    "    { \"$limit\" : 10 }\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    pct = float(a[\"count\"])/ct\n",
    "    print a[\"_id\"] + \": \" + \"{:,}\".format(a[\"count\"]) + \", \" + \"{:.1%}\".format(pct) + \" of total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with only one entry: 210\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "        {\"$group\" : {\"_id\" : \"$created.user\",\n",
    "                   \"count\" : {\"$sum\" : 1}}},\n",
    "        {\"$match\" : { \"count\" : 1 } },\n",
    "        {\"$group\" : { \"_id\" : \"$count\",\n",
    "                    \"count\" : {\"$sum\" : 1}}}\n",
    "    ]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print \"Number of users with only one entry: \" + str(a[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1663407\n",
      "school 2152\n",
      "parking 1952\n",
      "place_of_worship 595\n",
      "kindergarten 521\n",
      "restaurant 391\n",
      "fast_food 361\n",
      "fuel 327\n",
      "fire_station 314\n",
      "police 199\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "            {\"$group\" : {\"_id\" : \"$amenity\",\n",
    "                        \"count\" : { \"$sum\" : 1 }}},\n",
    "            {\"$sort\" : { \"count\" : -1}},\n",
    "            {\"$limit\" : 10}\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print a[\"_id\"], a[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1668086\n",
      "pitch 1901\n",
      "park 1008\n",
      "swimming_pool 703\n",
      "golf_course 103\n",
      "playground 95\n",
      "sports_centre 73\n",
      "stadium 56\n",
      "marina 41\n",
      "track 39\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "            {\"$group\" : {\"_id\" : \"$leisure\",\n",
    "                        \"count\" : { \"$sum\" : 1 }}},\n",
    "            {\"$sort\" : { \"count\" : -1}},\n",
    "            {\"$limit\" : 10}\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print a[\"_id\"], a[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1652418\n",
      "Weston 18251\n",
      "Miami 336\n",
      "Fort Lauderdale 188\n",
      "Miami Beach 137\n",
      "Wellington 102\n",
      "Royal Palm Beach 77\n",
      "Boca Raton 68\n",
      "Pelican Lake 57\n",
      "West Palm Beach 53\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "            {\"$group\" : {\"_id\" : \"$address.city\",\n",
    "                        \"count\" : { \"$sum\" : 1 }}},\n",
    "            {\"$sort\" : { \"count\" : -1}},\n",
    "            {\"$limit\" : 10 }\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print a[\"_id\"], a[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataabierta 18237\n",
      "ernestocd 8\n",
      "Auction123 1\n",
      "bladdiaz 1\n",
      "adjuva 1\n",
      "MountainAddict 1\n",
      "grouper 1\n",
      "mentor 1\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "            { \"$match\" : { \"address.city\" : \"Weston\"}},\n",
    "            {\"$group\" : {\"_id\" : \"$created.user\",\n",
    "                        \"count\" : { \"$sum\" : 1 }}},\n",
    "            {\"$sort\" : { \"count\" : -1}},\n",
    "            {\"$limit\" : 25 }\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print a[\"_id\"], a[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alejandro’s by Alejandro (Hair Salon) mentor\n",
      "Fix Apple Now grouper\n",
      "165 bladdiaz\n",
      "Ultimate Software ernestocd\n",
      "Ultimate Software ernestocd\n",
      "Auction123.com Auction123\n",
      "Falcon Cove Middle School ernestocd\n",
      "Everglades Elementary School ernestocd\n",
      "Sawgrass Recreation Park adjuva\n"
     ]
    }
   ],
   "source": [
    "cg = db.miami.find({\"address.city\":\"Weston\"})\n",
    "for c in cg:\n",
    "    if \"name\" in c:\n",
    "        print c[\"name\"], c[\"created\"][\"user\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'University of Miami Police Department'\n",
      "u'Miami Spine & Posture Clinic'\n",
      "u'Fritz & Franz Bierhaus'\n",
      "u\"Swensen's\"\n",
      "u'Law Office of Ferdie and Lones Chartered'\n",
      "u'Biltmore Golf Course  ,'\n",
      "u'Granada Golf Course  ,'\n",
      "u'Village of Merrick Park'\n",
      "u'Villa Capri'\n",
      "u'Otto G. Richter Library'\n",
      "u'Holiday Inn Coral Gables'\n",
      "u'7-Eleven'\n",
      "u'Coral Gables Art Cinema'\n",
      "u'Books and Books Bookstore/Cafe'\n",
      "u'Walsh Tower'\n",
      "u'Rosborough Tower'\n",
      "u'Hecht Residential College Commons'\n",
      "u'Pentland Tower'\n",
      "u'McDonald Tower'\n",
      "u'Eaton Residential College'\n",
      "u'Whitten University Center'\n",
      "u'Hecht-Stanford Dining Hall'\n",
      "u'Stanford Residential College Commons'\n",
      "u'Donna E. Shalala Student Center'\n",
      "u'Gusman Concert Hall'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "cg = db.miami.find({\"address.city\":\"Coral Gables\"})\n",
    "for c in cg:\n",
    "    if \"name\" in c:\n",
    "        pprint.pprint(c[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Latitude: 25.2910022\n",
      "Max Latitude: 26.9119981\n",
      "Min Longitude: -80.682989\n",
      "Max Longitude: -79.8064041\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "            {\"$unwind\" : \"$pos\" },\n",
    "            {\"$group\" : {\"_id\" : \"$_id\",\n",
    "                        \"lat\" : { \"$first\" : \"$pos\" }}},\n",
    "            {\"$project\" : {\"_id\":0, \"lat\":1}},\n",
    "            {\"$group\" : {\"_id\" : \"$_id\",\n",
    "                        \"minLat\" : { \"$min\" : \"$lat\"},\n",
    "                        \"maxLat\" : { \"$max\" : \"$lat\"}}}\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print \"Min Latitude: \" + str(a[\"minLat\"])\n",
    "    print \"Max Latitude: \" + str(a[\"maxLat\"])\n",
    "pipeline = [\n",
    "            {\"$unwind\" : \"$pos\" },\n",
    "            {\"$group\" : {\"_id\" : \"$_id\",\n",
    "                        \"lon\" : { \"$last\" : \"$pos\" }}},\n",
    "            {\"$project\" : {\"_id\":0, \"lon\":1}},\n",
    "            {\"$group\" : {\"_id\" : \"$_id\",\n",
    "                        \"minLon\" : { \"$min\" : \"$lon\"},\n",
    "                        \"maxLon\" : { \"$max\" : \"$lon\"}}}\n",
    "]\n",
    "agg = db.miami.aggregate(pipeline)\n",
    "for a in agg:\n",
    "    print \"Min Longitude: \" + str(a[\"minLon\"])\n",
    "    print \"Max Longitude: \" + str(a[\"maxLon\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
